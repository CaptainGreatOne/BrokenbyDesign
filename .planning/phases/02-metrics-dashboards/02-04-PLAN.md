---
phase: 02-metrics-dashboards
plan: 04
type: execute
wave: 3
depends_on: ["02-03"]
files_modified:
  - grafana/provisioning/datasources/prometheus.yml
  - grafana/provisioning/dashboards/dashboards.yml
  - grafana/dashboards/service-overview.json
  - grafana/dashboards/container-resources.json
  - grafana/dashboards/host-metrics.json
  - docker-compose.yml
autonomous: true

must_haves:
  truths:
    - "Grafana launches with Prometheus datasource pre-configured on first boot"
    - "Service Overview dashboard shows request rate, error rate, and p95/p99 latency panels"
    - "Container Resources dashboard shows per-container CPU and memory from cAdvisor"
    - "Host Metrics dashboard shows host CPU, memory, and disk from Node Exporter"
    - "Dashboards populate with data within 60 seconds of startup"
    - "Grafana is accessible on port 3001"
  artifacts:
    - path: "grafana/provisioning/datasources/prometheus.yml"
      provides: "Auto-provisioned Prometheus datasource"
      contains: "type: prometheus"
    - path: "grafana/provisioning/dashboards/dashboards.yml"
      provides: "Dashboard provider pointing to /var/lib/grafana/dashboards"
      contains: "path: /var/lib/grafana/dashboards"
    - path: "grafana/dashboards/service-overview.json"
      provides: "Service metrics dashboard with request rate, error rate, latency panels"
      contains: "http_requests_total"
    - path: "grafana/dashboards/container-resources.json"
      provides: "Container CPU and memory panels from cAdvisor"
      contains: "container_memory_usage_bytes"
    - path: "grafana/dashboards/host-metrics.json"
      provides: "Host CPU, memory, disk panels from Node Exporter"
      contains: "node_cpu_seconds_total"
    - path: "docker-compose.yml"
      provides: "Grafana service definition with provisioning volume mounts"
      contains: "grafana/grafana"
  key_links:
    - from: "docker-compose.yml"
      to: "grafana/provisioning"
      via: "volume mount"
      pattern: "./grafana/provisioning:/etc/grafana/provisioning"
    - from: "docker-compose.yml"
      to: "grafana/dashboards"
      via: "volume mount"
      pattern: "./grafana/dashboards:/var/lib/grafana/dashboards"
    - from: "grafana/provisioning/datasources/prometheus.yml"
      to: "prometheus:9090"
      via: "datasource URL"
      pattern: "http://prometheus:9090"
    - from: "grafana/dashboards/service-overview.json"
      to: "Prometheus"
      via: "datasource reference in panels"
      pattern: "Prometheus"
---

<objective>
Deploy Grafana with pre-provisioned Prometheus datasource and three pre-built dashboards: Service Overview (request rates, errors, latency), Container Resources (CPU/memory from cAdvisor), and Host Metrics (system metrics from Node Exporter).

Purpose: Grafana is the learner-facing visualization layer. Pre-built dashboards mean the learner sees useful data immediately without manual setup, fulfilling the "remove setup tax" philosophy.
Output: Grafana running on port 3001 with 3 pre-built dashboards that populate automatically.
</objective>

<execution_context>
@/home/user/.claude/get-shit-done/workflows/execute-plan.md
@/home/user/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-metrics-dashboards/02-RESEARCH.md
@.planning/phases/02-metrics-dashboards/02-03-SUMMARY.md

# Existing compose file to modify
@docker-compose.yml
@prometheus/prometheus.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Grafana provisioning config and add to Docker Compose</name>
  <files>
    grafana/provisioning/datasources/prometheus.yml
    grafana/provisioning/dashboards/dashboards.yml
    docker-compose.yml
  </files>
  <action>
1. Create directory structure:
   ```
   grafana/
     provisioning/
       datasources/
         prometheus.yml
       dashboards/
         dashboards.yml
     dashboards/
       (dashboard JSONs go here -- created in Task 2)
   ```

2. Create `grafana/provisioning/datasources/prometheus.yml`:
   ```yaml
   apiVersion: 1

   datasources:
     - name: Prometheus
       type: prometheus
       access: proxy
       url: http://prometheus:9090
       isDefault: true
       editable: false
       jsonData:
         timeInterval: 15s
   ```

3. Create `grafana/provisioning/dashboards/dashboards.yml`:
   ```yaml
   apiVersion: 1

   providers:
     - name: 'default'
       orgId: 1
       folder: ''
       type: file
       disableDeletion: false
       updateIntervalSeconds: 10
       allowUiUpdates: true
       options:
         path: /var/lib/grafana/dashboards
   ```

4. Add grafana service to `docker-compose.yml`:
   - image: `grafana/grafana:12.3.0`
   - container_name: grafana
   - environment:
     - `GF_SECURITY_ADMIN_USER=admin`
     - `GF_SECURITY_ADMIN_PASSWORD=admin`
     - `GF_PATHS_PROVISIONING=/etc/grafana/provisioning`
     - `GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/service-overview.json`
   - volumes:
     - `./grafana/provisioning:/etc/grafana/provisioning:ro`
     - `./grafana/dashboards:/var/lib/grafana/dashboards:ro`
     - `grafana-data:/var/lib/grafana`
   - ports: `3001:3000` (map to 3001 because web-gateway already uses port 3000 on the host... actually web-gateway does NOT expose port 3000 on host -- nginx proxies to it. But to avoid confusion with the internal port, use 3001 for Grafana)
   - depends_on:
     - prometheus: condition: service_healthy
   - healthcheck: `test: ["CMD-SHELL", "wget --spider -q http://localhost:3000/api/health || exit 1"]`, interval: 5s, timeout: 3s, retries: 5, start_period: 20s
   - restart: unless-stopped
   - deploy resources: memory limit 512M, cpus 1.0, memory reservation 256M, cpus 0.5
   - logging: json-file, max-size 10m, max-file 3

5. Add `grafana-data` to the volumes section at the bottom of docker-compose.yml.

IMPORTANT: Grafana depends_on prometheus with `condition: service_healthy`. This prevents the Pitfall 1 startup race condition identified in research -- Grafana will wait for Prometheus to be ready before provisioning the datasource.

IMPORTANT: Use port 3001 on the host for Grafana. While web-gateway's port 3000 is not mapped to the host directly (nginx uses it internally), using 3001 is clearer and avoids any future conflicts.
  </action>
  <verify>
Run `docker compose config` to validate. Verify grafana/provisioning/datasources/prometheus.yml exists. Verify grafana/provisioning/dashboards/dashboards.yml exists. Verify docker-compose.yml has grafana service with correct volume mounts and depends_on.
  </verify>
  <done>
Grafana provisioning directory structure created. Prometheus datasource auto-configured. Dashboard provider points to /var/lib/grafana/dashboards. Grafana service in docker-compose.yml with healthcheck, depends_on prometheus (service_healthy), port 3001, resource limits, and provisioning volume mounts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create pre-built Grafana dashboard JSON files</name>
  <files>
    grafana/dashboards/service-overview.json
    grafana/dashboards/container-resources.json
    grafana/dashboards/host-metrics.json
  </files>
  <action>
Create three Grafana dashboard JSON files. Each must be a valid Grafana dashboard JSON (not wrapped in a `{"dashboard": ...}` envelope -- file-based provisioning uses the raw dashboard object).

**1. `grafana/dashboards/service-overview.json` -- Service Overview Dashboard**

This is the primary dashboard learners will use. Panels:

Row 1 - Service Health (4 panels, each 6w x 4h stat panels):
- **Services Up**: `up{job=~"web-gateway|order-api|fulfillment-worker"}` -- stat panel showing UP count
- **Total Request Rate**: `sum(rate(http_requests_total[5m]))` -- stat panel showing req/s
- **Error Rate %**: `sum(rate(http_requests_total{status_code=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) * 100` -- stat panel, red threshold at >5%
- **Orders Created**: `sum(orders_created_total)` or `sum(orders_processed_total{status="success"})` -- stat panel

Row 2 - Request Rates (2 panels, each 12w x 8h):
- **Request Rate by Service**: time series, `sum(rate(http_requests_total[5m])) by (job)`, legend `{{job}}`
- **Request Rate by Status Code**: time series, `sum(rate(http_requests_total[5m])) by (status_code)`, legend `{{status_code}}`

Row 3 - Latency (2 panels, each 12w x 8h):
- **p95 Latency by Service**: time series, `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))`, yaxis in seconds, legend `{{job}} p95`
- **p99 Latency by Service**: time series, `histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))`, yaxis in seconds, legend `{{job}} p99`

Row 4 - Error Tracking (2 panels, each 12w x 8h):
- **Error Rate by Service**: time series, `sum(rate(http_requests_total{status_code=~"5.."}[5m])) by (job)`, legend `{{job}}`
- **Fulfillment Processing**: time series showing `rate(orders_processed_total[5m])` by status label, legend `{{status}}`

Dashboard settings: title "Service Overview", tags ["prometheus", "services"], auto-refresh 10s, time range "now-1h" to "now", timezone "browser". Set `uid` to `service-overview` for stable linking.

NOTE: For gRPC metrics from order-api, use `grpc_requests_total` and `grpc_request_duration_seconds_bucket` in the queries. The web-gateway uses `http_requests_total` and `http_request_duration_seconds_bucket`. Use `{job=~"web-gateway|order-api|fulfillment-worker"}` to aggregate across services where metric names match, or separate panels where they differ.

Actually, to keep queries consistent across all services and simplify the dashboard, use a PromQL union approach: for request rate panels, show web-gateway HTTP metrics and order-api gRPC metrics in the same chart using separate targets per panel. Use `job` label to distinguish.

For the "Request Rate by Service" panel, use multiple targets:
- Target A: `sum(rate(http_requests_total{job="web-gateway"}[5m]))` legend "web-gateway"
- Target B: `sum(rate(grpc_requests_total{job="order-api"}[5m]))` legend "order-api"
- Target C: `sum(rate(orders_processed_total{job="fulfillment-worker"}[5m]))` legend "fulfillment-worker"

Similarly for latency, use separate targets for each service's histogram.

**2. `grafana/dashboards/container-resources.json` -- Container Resources Dashboard**

Panels (using cAdvisor metrics):

Row 1 - Overview (4 stat panels, 6w x 4h each):
- **Total Containers**: `count(container_last_seen{name=~".+"})` -- stat
- **Total CPU Usage**: `sum(rate(container_cpu_usage_seconds_total{name=~".+"}[5m])) * 100` -- stat, unit: percent
- **Total Memory**: `sum(container_memory_usage_bytes{name=~".+"}) / 1024 / 1024 / 1024` -- stat, unit: GB
- **Network RX Rate**: `sum(rate(container_network_receive_bytes_total{name=~".+"}[5m]))` -- stat, unit: bytes/s

Row 2 - CPU (1 panel, 24w x 8h):
- **CPU Usage by Container**: time series, `rate(container_cpu_usage_seconds_total{name=~".+"}[5m]) * 100`, legend `{{name}}`, unit: percent

Row 3 - Memory (1 panel, 24w x 8h):
- **Memory Usage by Container**: time series, `container_memory_usage_bytes{name=~".+"} / 1024 / 1024`, legend `{{name}}`, unit: MB

Row 4 - Network (2 panels, 12w x 8h each):
- **Network Receive**: time series, `rate(container_network_receive_bytes_total{name=~".+"}[5m])`, legend `{{name}}`, unit: bytes/s
- **Network Transmit**: time series, `rate(container_network_transmit_bytes_total{name=~".+"}[5m])`, legend `{{name}}`, unit: bytes/s

Dashboard settings: title "Container Resources", tags ["prometheus", "cadvisor", "containers"], auto-refresh 10s, time range "now-1h", uid "container-resources".

**3. `grafana/dashboards/host-metrics.json` -- Host Metrics Dashboard**

Panels (using Node Exporter metrics):

Row 1 - Overview (4 stat panels, 6w x 4h each):
- **CPU Usage %**: `100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)` -- stat, unit: percent
- **Memory Usage %**: `(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100` -- stat, unit: percent
- **Disk Usage %**: `(1 - node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100` -- stat, unit: percent
- **System Load**: `node_load1` -- stat

Row 2 - CPU Detail (1 panel, 24w x 8h):
- **CPU Usage by Mode**: stacked time series, `avg(rate(node_cpu_seconds_total[5m])) by (mode) * 100`, legend `{{mode}}`, unit: percent

Row 3 - Memory Detail (1 panel, 24w x 8h):
- **Memory Usage**: time series with multiple targets:
  - `node_memory_MemTotal_bytes / 1024 / 1024 / 1024` legend "Total"
  - `(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / 1024 / 1024 / 1024` legend "Used"
  - `node_memory_MemAvailable_bytes / 1024 / 1024 / 1024` legend "Available"
  - Unit: GB

Row 4 - Disk (2 panels, 12w x 8h each):
- **Disk Space Usage**: bar gauge, `node_filesystem_avail_bytes{fstype!~"tmpfs|devtmpfs"} / node_filesystem_size_bytes * 100`, legend `{{mountpoint}}`
- **Disk I/O**: time series, read `rate(node_disk_read_bytes_total[5m])`, write `rate(node_disk_written_bytes_total[5m])`, unit: bytes/s

Dashboard settings: title "Host Metrics", tags ["prometheus", "node-exporter", "host"], auto-refresh 10s, time range "now-1h", uid "host-metrics".

**JSON format notes for all dashboards:**
- Use Grafana dashboard JSON schema (id: null, version: 0 for new dashboards)
- Each panel needs: id, title, type ("timeseries", "stat", "bargauge"), datasource `{"type": "prometheus", "uid": "${DS_PROMETHEUS}"}` -- actually for file provisioning use `{"type": "prometheus", "uid": "PBFA97CFB590B2093"}` -- no, just use the datasource name: `"datasource": "Prometheus"` (simplest approach for file provisioning)
- Actually, the most reliable approach for provisioned dashboards: use `"datasource": {"type": "prometheus", "uid": "prometheus"}` and set the datasource uid in the provisioning YAML. Update `grafana/provisioning/datasources/prometheus.yml` to include `uid: prometheus`.
- Set `"editable": true` so learners can explore and modify
- Set `"schemaVersion"` to a recent version (e.g., 39)

IMPORTANT: Dashboard JSON files should be the raw dashboard object, NOT wrapped in `{"dashboard": {...}}`. File-based provisioning reads the raw format.
  </action>
  <verify>
Validate each JSON file is valid JSON: `python3 -c "import json; json.load(open('grafana/dashboards/service-overview.json'))"` (and same for the other two). Verify each dashboard has panels with PromQL expressions. Verify the provisioning datasource YAML has `uid: prometheus` matching the dashboard datasource references. Run `docker compose config` to validate the full compose file.
  </verify>
  <done>
Three dashboard JSON files created and valid. Service Overview shows request rates, error rates, p95/p99 latency across all services. Container Resources shows per-container CPU, memory, and network from cAdvisor. Host Metrics shows host CPU, memory, disk, and I/O from Node Exporter. Grafana datasource provisioning includes uid for dashboard references. All dashboards have auto-refresh 10s and 1-hour time range.
  </done>
</task>

</tasks>

<verification>
1. `docker compose config` validates without errors
2. All 3 dashboard JSON files are valid JSON
3. Grafana provisioning datasource references `http://prometheus:9090`
4. Dashboard provider points to `/var/lib/grafana/dashboards`
5. Grafana service depends on prometheus (service_healthy)
6. Grafana accessible on host port 3001
7. Dashboard queries use correct metric names matching instrumented services
</verification>

<success_criteria>
- Grafana auto-provisions Prometheus datasource on first boot
- 3 pre-built dashboards load automatically
- Service Overview dashboard has request rate, error rate, p95/p99 latency panels
- Container Resources dashboard has CPU and memory panels from cAdvisor
- Host Metrics dashboard has CPU, memory, disk panels from Node Exporter
- Dashboards use 10s auto-refresh and 1-hour time range
- Dashboards populate with data within 60 seconds of environment startup (15s scrape + 10s refresh)
</success_criteria>

<output>
After completion, create `.planning/phases/02-metrics-dashboards/02-04-SUMMARY.md`
</output>
