---
phase: 03-centralized-logging
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - services/order-api/src/logger.py
  - services/order-api/src/server.py
  - services/order-api/src/db.py
  - services/order-api/src/redis_queue.py
autonomous: true

must_haves:
  truths:
    - "order-api outputs plain text logs (not JSON) to stdout"
    - "Every log line includes timestamp, level, service name, handler, order/request ID, and message"
    - "order-api occasionally emits realistic WARN and ERROR logs during normal operation"
  artifacts:
    - path: "services/order-api/src/logger.py"
      provides: "Plain text log formatter with handler/ID extraction"
      contains: "def log"
  key_links:
    - from: "services/order-api/src/server.py"
      to: "services/order-api/src/logger.py"
      via: "log function calls with handler kwarg"
      pattern: "handler="
---

<objective>
Rewrite order-api logging from JSON to plain text format and add realistic error patterns.

Purpose: order-api must emit plain text logs matching the project-wide format `[timestamp] [level] [service] [handler] [id] [details] message` for Loki collection. Realistic database and Redis errors during normal operation give the learner genuine error patterns to explore.

Output: Updated logger.py with plain text formatting. All callers updated with consistent `handler` keyword argument. Occasional simulated database connection issues and retry patterns.
</objective>

<execution_context>
@/home/user/.claude/get-shit-done/workflows/execute-plan.md
@/home/user/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-centralized-logging/03-RESEARCH.md
@services/order-api/src/logger.py
@services/order-api/src/server.py
@services/order-api/src/db.py
@services/order-api/src/redis_queue.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite logger.py to plain text format</name>
  <files>services/order-api/src/logger.py</files>
  <action>
Replace the JSON logger (`json_log`) with a plain text formatter. Keep the function name `json_log` for now to minimize caller changes (just the output format changes), OR rename to `log` and update all imports -- either approach is fine as long as all callers work.

Recommended approach: rename to `log` and add backward-compatible alias `json_log = log`.

The log format must be:

```
{timestamp} {LEVEL} order-api {handler} {id} {key=value pairs} {message}
```

Implementation:
1. `def log(level: str, message: str, **kwargs)` -- same kwargs interface
2. Extract `handler` from kwargs (default empty string)
3. Extract ID: check for `order_id`, `req_id`, `correlation_id` in priority order. Format as `order={value}` or `req={value}` or `corr={value}`
4. Remaining kwargs become `key=value` pairs
5. Level uppercase, padded to 5 chars
6. Print to stdout with flush=True (critical for Docker log capture)
7. No JSON import needed -- remove `import json`

Example outputs:
- `2026-02-08T14:32:01.123Z INFO  order-api CreateOrder order=abc123 user=user42 Processing new order`
- `2026-02-08T14:32:02.456Z ERROR order-api CreateOrder order=abc123 retries=3 error=connection timeout Max retries exceeded`
- `2026-02-08T14:32:03.789Z WARN  order-api DatabasePool attempt=2 Database connection pool exhausted, waiting`

Also add convenience functions: `info(message, **kwargs)`, `warn(message, **kwargs)`, `error(message, **kwargs)` that call `log` with the appropriate level. Keep `json_log` as an alias for backward compatibility.
  </action>
  <verify>
Read logger.py and confirm:
- No `json.dumps` or `json` import
- Output format matches `timestamp LEVEL service handler id details message`
- Uses `print(..., flush=True)` for stdout
  </verify>
  <done>logger.py outputs plain text in the specified format with handler/ID extraction from kwargs</done>
</task>

<task type="auto">
  <name>Task 2: Update all callers with handler metadata and add realistic error patterns</name>
  <files>
    services/order-api/src/server.py
    services/order-api/src/db.py
    services/order-api/src/redis_queue.py
  </files>
  <action>
Update all logger calls to include a `handler` keyword argument. Add realistic error simulation for the learner.

**server.py** - Update gRPC servicer methods:
- `CreateOrder`: use `handler='CreateOrder'`
- `GetOrder`: use `handler='GetOrder'`
- `ListOrders`: use `handler='ListOrders'`
- Startup/shutdown: use `handler='Server'`
- Metrics server: use `handler='MetricsServer'`
- Update import if function was renamed (add `from logger import log` or keep `json_log`)

Add realistic error simulation in CreateOrder:
- ~3% chance: log WARN "Database connection pool pressure detected" with `handler='CreateOrder'` before proceeding normally
- ~2% chance: log WARN "Slow query detected" with `handler='CreateOrder'` and `duration_ms=random(500-2000)` after successful insert

**db.py** - Update database operations:
- Use `handler='DatabasePool'` for connection management
- Use `handler='OrdersTable'` for order CRUD operations
- Use `handler='ProductsTable'` for product lookups

Add realistic error simulation:
- ~2% chance in `create_order`: log WARN "Connection pool wait time elevated" with `wait_ms=random(100-500)` before proceeding

**redis_queue.py** - Update Redis operations:
- Use `handler='RedisQueue'` for queue operations
- Use `handler='RedisCache'` for cache operations
- Use `handler='RedisConnection'` for connection management

Add realistic error simulation:
- ~3% chance in `enqueue_fulfillment`: log WARN "Redis pipeline latency elevated" with `latency_ms=random(50-200)` before proceeding

Important: Error simulation is log-only. The actual operations proceed normally. Use `import random` and `random.random()` for probability checks. These simulate the kind of intermittent issues seen in production systems.
  </action>
  <verify>
Run: `grep -r "handler=" services/order-api/src/ | head -20` to confirm handler fields present.
Run: `grep -r "random.random" services/order-api/src/` to confirm error simulation added.
Run: `grep -r "json.dumps" services/order-api/src/logger.py` returns no results.
  </verify>
  <done>All order-api logger calls include handler metadata, and realistic WARN simulation produces occasional log entries (~3% pool pressure, ~2% slow queries, ~3% Redis latency)</done>
</task>

</tasks>

<verification>
1. Read logger.py -- confirm plain text format, no JSON serialization
2. Grep for `handler=` across all order-api src files -- every logger call should have it
3. Grep for `random.random` -- confirm realistic error patterns exist
4. Backward compatibility maintained (json_log alias or import updates)
</verification>

<success_criteria>
- order-api logger outputs plain text in format: `timestamp LEVEL order-api handler id details message`
- All callers include `handler` keyword argument
- Realistic WARN simulation produces occasional log entries during normal traffic
- No JSON output from logger
- Service functionality unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/03-centralized-logging/03-02-SUMMARY.md`
</output>
