---
phase: 01-foundation-services
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - services/order-api/src/server.py
  - services/order-api/src/db.py
  - services/order-api/src/redis_queue.py
  - services/order-api/src/logger.py
  - services/order-api/Dockerfile
  - services/order-api/requirements.txt
autonomous: true

must_haves:
  truths:
    - "Order API starts and listens on gRPC port 50051"
    - "CreateOrder RPC saves order to PostgreSQL and enqueues fulfillment message to Redis"
    - "GetOrder RPC retrieves order from PostgreSQL by ID"
    - "ListOrders RPC returns recent orders from PostgreSQL"
    - "Product data is cached in Redis with TTL"
    - "All operations emit structured JSON logs with correlation_id"
  artifacts:
    - path: "services/order-api/src/server.py"
      provides: "gRPC server implementing OrderService"
      contains: "add_OrderServiceServicer_to_server"
    - path: "services/order-api/src/db.py"
      provides: "PostgreSQL connection pool and query functions"
      contains: "psycopg"
    - path: "services/order-api/src/redis_queue.py"
      provides: "Redis cache and queue operations"
      contains: "lpush.*fulfillment_queue"
    - path: "services/order-api/src/logger.py"
      provides: "Structured JSON logging"
      contains: "json.dumps"
    - path: "services/order-api/Dockerfile"
      provides: "Container image for order-api"
      contains: "FROM python"
    - path: "services/order-api/requirements.txt"
      provides: "Python dependencies"
      contains: "grpcio"
  key_links:
    - from: "services/order-api/src/server.py"
      to: "services/order-api/src/db.py"
      via: "import for database operations"
      pattern: "from.*db import|import db"
    - from: "services/order-api/src/server.py"
      to: "services/order-api/src/redis_queue.py"
      via: "import for queue publishing"
      pattern: "from.*redis_queue import|import redis_queue"
    - from: "services/order-api/src/server.py"
      to: "proto/order.proto"
      via: "generated protobuf code"
      pattern: "order_pb2_grpc"
    - from: "services/order-api/src/db.py"
      to: "postgres:5432"
      via: "psycopg connection"
      pattern: "psycopg.*connect|ConnectionPool"
---

<objective>
Build the Order API Python microservice -- a gRPC server that implements the OrderService contract, queries PostgreSQL for order and product data, publishes fulfillment messages to Redis queue, and caches product data in Redis.

Purpose: The Order API is the central business logic service. It receives gRPC calls from the Web Gateway, persists orders to PostgreSQL, and triggers async fulfillment by publishing to Redis. This service demonstrates Python gRPC server patterns and dual-purpose Redis usage (cache + queue).

Output: Complete services/order-api/ directory with working gRPC server, database layer, Redis operations, structured logging, Dockerfile, and requirements.txt
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-services/01-CONTEXT.md
@.planning/phases/01-foundation-services/01-RESEARCH.md
@.planning/phases/01-foundation-services/01-01-SUMMARY.md
@proto/order.proto
@docker-compose.yml
@.env
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Order API database layer and Redis operations</name>
  <files>
    services/order-api/src/db.py
    services/order-api/src/redis_queue.py
    services/order-api/src/logger.py
    services/order-api/requirements.txt
  </files>
  <action>
    Create the supporting modules for the Order API service:

    1. **services/order-api/src/logger.py** - Structured JSON logging:
       - Function `json_log(level, message, **kwargs)` that prints JSON to stdout
       - Fields: timestamp (ISO 8601 UTC), level, service ("order-api"), message, plus any kwargs
       - Correlation ID should be passed via kwargs when available
       - Use print(json.dumps(entry)) for output (Docker captures stdout)

    2. **services/order-api/src/db.py** - PostgreSQL database layer using psycopg3:
       - Use `psycopg_pool.ConnectionPool` for connection pooling (min_size=2, max_size=10)
       - Connection string from environment variables: POSTGRES_HOST, POSTGRES_PORT, POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB (from .env)
       - Functions:
         - `init_pool()` - Create and return connection pool. Retry connection up to 5 times with 2-second delay (handles startup ordering edge cases).
         - `create_order(product_id, quantity)` - INSERT INTO orders, return order dict with id and status
         - `get_order(order_id)` - SELECT from orders by id, return order dict or None
         - `list_orders(limit=10)` - SELECT recent orders with LIMIT, return list of order dicts
         - `get_product(product_id)` - SELECT from products by id, return product dict or None
         - `update_order_status(order_id, status)` - UPDATE orders SET status, updated_at
       - All functions use parameterized queries (never string formatting for SQL)
       - Return dictionaries (not tuples) for clean API boundaries
       - Log database operations using logger.py

    3. **services/order-api/src/redis_queue.py** - Redis dual-purpose operations:
       - Initialize Redis client from REDIS_URL environment variable
       - Queue functions:
         - `enqueue_fulfillment(order_id, product_id, quantity, correlation_id)` - LPUSH JSON message to "fulfillment_queue"
         - Message format: {order_id, product_id, quantity, correlation_id, timestamp}
       - Cache functions:
         - `get_cached_product(product_id)` - GET from "product:{product_id}", return parsed JSON or None
         - `cache_product(product_id, product_data, ttl=300)` - SETEX "product:{product_id}" with JSON serialization, 5-minute TTL
         - `invalidate_product_cache(product_id)` - DELETE "product:{product_id}"
       - Connection retry logic: attempt connection up to 5 times with 2-second delay
       - Log all Redis operations using logger.py

    4. **services/order-api/requirements.txt**:
       - grpcio>=1.62.0
       - grpcio-tools>=1.62.0
       - psycopg[binary]>=3.1.0
       - psycopg-pool>=3.1.0
       - redis>=5.0.0
  </action>
  <verify>
    - All 4 files exist at specified paths
    - requirements.txt lists grpcio, psycopg, redis
    - db.py contains parameterized queries (using %s or $1 placeholders, NOT f-strings for SQL)
    - redis_queue.py contains lpush and setex operations
    - logger.py outputs JSON format
  </verify>
  <done>
    Database layer connects to PostgreSQL via pool with retry logic. Redis module provides both queue publishing (LPUSH to fulfillment_queue) and caching (GET/SETEX/DELETE product keys). Logger outputs structured JSON. All dependencies declared in requirements.txt.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Order API gRPC server and Dockerfile</name>
  <files>
    services/order-api/src/server.py
    services/order-api/src/__init__.py
    services/order-api/Dockerfile
  </files>
  <action>
    Create the main gRPC server and Docker container for the Order API:

    1. **Generate protobuf Python code** first:
       - The Dockerfile should generate proto files during build (or server.py should reference them from a generated location)
       - Proto generation command: `python -m grpc_tools.protoc -I../../proto --python_out=./src --grpc_python_out=./src ../../proto/order.proto`
       - Alternative: Generate at build time in Dockerfile. Copy proto/order.proto into the image, run protoc, then the generated files are available.

    2. **services/order-api/src/server.py** - gRPC server implementation:
       - Import generated order_pb2 and order_pb2_grpc modules
       - Class `OrderServicer(order_pb2_grpc.OrderServiceServicer)`:
         - `CreateOrder(request, context)`:
           - Extract correlation_id from gRPC metadata (key: "x-correlation-id"), generate UUID if missing
           - Validate product exists (check cache first via redis_queue.get_cached_product, then DB via db.get_product, cache on miss)
           - If product not found, set gRPC status to NOT_FOUND and abort
           - Call db.create_order(product_id, quantity)
           - Call redis_queue.enqueue_fulfillment(order_id, product_id, quantity, correlation_id)
           - Log the operation with correlation_id
           - Return CreateOrderResponse with order_id and status "pending"
         - `GetOrder(request, context)`:
           - Extract correlation_id from metadata
           - Call db.get_order(order_id)
           - If not found, set gRPC status NOT_FOUND and abort
           - Return Order message
           - Log with correlation_id
         - `ListOrders(request, context)`:
           - Extract correlation_id from metadata
           - Call db.list_orders(limit=request.limit or 10)
           - Return ListOrdersResponse with list of Order messages
           - Log with correlation_id
       - `serve()` function:
         - Initialize database pool (db.init_pool())
         - Create gRPC server with ThreadPoolExecutor(max_workers=10)
         - Add OrderServicer to server
         - Listen on [::]:50051 (from GRPC_PORT env var, default 50051)
         - Log "Order API started on port 50051"
         - server.wait_for_termination() with graceful shutdown on SIGTERM
       - `if __name__ == '__main__': serve()`
       - Handle SIGTERM gracefully (important for Docker): register signal handler that calls server.stop(grace=5)

    3. **services/order-api/src/__init__.py** - Empty file for Python package

    4. **services/order-api/Dockerfile**:
       - FROM python:3.12-slim
       - WORKDIR /app
       - COPY requirements.txt .
       - RUN pip install --no-cache-dir -r requirements.txt
       - COPY proto/ /app/proto/ (copy the shared proto directory)
       - RUN python -m grpc_tools.protoc -I/app/proto --python_out=/app/src --grpc_python_out=/app/src /app/proto/order.proto
       - COPY src/ /app/src/
       - EXPOSE 50051
       - CMD ["python", "-u", "src/server.py"]
       - Use -u flag for unbuffered stdout (so Docker captures logs immediately)
       - Note: The build context is set to repo root in docker-compose.yml (handled in Plan 01-01): `build: { context: ., dockerfile: services/order-api/Dockerfile }`. This Dockerfile assumes build context is repo root for proto/ access.
  </action>
  <verify>
    - server.py implements all 3 RPCs (CreateOrder, GetOrder, ListOrders)
    - server.py extracts correlation_id from gRPC metadata
    - server.py calls db and redis_queue modules
    - Dockerfile builds from python:3.12-slim and generates protobuf code
    - Docker build succeeds: `docker build -f services/order-api/Dockerfile .` (from repo root)
  </verify>
  <done>
    Order API gRPC server implements all 3 OrderService RPCs with correlation ID propagation. CreateOrder validates product, saves to DB, and enqueues fulfillment. GetOrder and ListOrders query PostgreSQL. Dockerfile generates protobuf code at build time and runs server with unbuffered output. Build context is repo root to access shared proto/.
  </done>
</task>

</tasks>

<verification>
- Docker build succeeds for order-api: `docker build -f services/order-api/Dockerfile -t order-api-test .`
- Python syntax check passes: `python -m py_compile services/order-api/src/server.py` (after installing deps)
- Requirements file includes all needed packages
- Server implements all 3 gRPC RPCs from the proto contract
- Database layer uses connection pooling and parameterized queries
- Redis layer provides both queue (LPUSH) and cache (SETEX/GET) operations
- All modules use structured JSON logging
</verification>

<success_criteria>
- Complete services/order-api/ directory with server.py, db.py, redis_queue.py, logger.py
- gRPC server implements CreateOrder, GetOrder, ListOrders
- CreateOrder flow: validate product -> save order -> enqueue fulfillment
- Correlation ID extracted from gRPC metadata and included in all logs
- Redis dual-purpose: product caching (SETEX) + fulfillment queue (LPUSH)
- PostgreSQL queries use connection pool and parameterized queries
- Dockerfile generates protobuf code at build time
- Docker image builds successfully
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-services/01-02-SUMMARY.md`
</output>
